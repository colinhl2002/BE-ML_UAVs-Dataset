{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9632e0c3",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">BE - ML</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82150020",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Unmanned Aerial Vehicles (UAVs), commonly known as drones, are increasingly being deployed for applications such as surveillance, delivery, environmental monitoring, and disaster management. However, the growing reliance on UAVs also exposes them to significant **cybersecurity risks**, as malicious actors can exploit vulnerabilities in their communication and control systems. To address this challenge, **intrusion detection systems (IDSs)** leveraging machine learning have become an active area of research.\n",
    "\n",
    "This project utilizes the **Cyber-Physical Dataset for UAVs Under Normal Operations and Cyberattacks**, developed by **Hassler, Mughal, and Ismail (2023)**. The dataset captures both **cyber and physical telemetry data** from UAVs under normal flight conditions and during four distinct types of cyberattacks:\n",
    "\n",
    "1. **De-authentication Denial-of-Service (DoS) attack**\n",
    "2. **Replay attack**\n",
    "3. **False Data Injection (FDI) attack**\n",
    "4. **Evil Twin attack**\n",
    "\n",
    "The dataset is composed of two major components:\n",
    "\n",
    "* **Cyber dataset** containing 37 features that describe network-level parameters and communication behavior.\n",
    "* **Physical dataset** containing 16 features that represent UAV motion dynamics such as altitude, velocity, position, and orientation.\n",
    "\n",
    "Each record in the dataset corresponds to either **benign (normal)** operation or one of the aforementioned **attack scenarios**, making it suitable for **binary (normal vs. malicious)** or **multi-class** classification tasks. The data were collected using a custom UAV testbed with real-time attack emulation and monitoring tools.\n",
    "\n",
    "\n",
    "**Reference:**\n",
    "Hassler, S. C., Mughal, U. A., & Ismail, M. (2023). *Cyber-Physical Intrusion Detection System for Unmanned Aerial Vehicles*. IEEE Transactions on Intelligent Transportation Systems.\n",
    "Available via GitHub: [https://github.com/uamughal/UAVs-Dataset-Under-Normal-and-Cyberattacks](https://github.com/uamughal/UAVs-Dataset-Under-Normal-and-Cyberattacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e82d1",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "\n",
    "In this notebook, we aim to design and evaluate intelligent **cyber-physical intrusion detection systems (IDS)** capable of detecting abnormal UAV behaviors in real time.\n",
    "To achieve this, the problem is approached in **two complementary ways**:\n",
    "\n",
    "### 1. **Supervised Classification**\n",
    "\n",
    "In this first stage, the task is treated as a standard **supervised learning problem** where each observation is labeled as *benign* or *malicious*.\n",
    "We train and compare two widely used machine learning algorithms:\n",
    "\n",
    "* **Support Vector Machine (SVM)** ‚Äî a robust classifier that finds optimal separating hyperplanes.\n",
    "* **Random Forest (RF)** ‚Äî an ensemble method that combines multiple decision trees to improve generalization and interpretability.\n",
    "\n",
    "These models are evaluated in terms of precision, recall, and F1-score across different datasets (Cyber, Physical, and Cyber-Physical).\n",
    "\n",
    "### 2. **Unsupervised Anomaly Detection**\n",
    "\n",
    "In the second stage, the problem is reformulated as an **anomaly detection task**, simulating a real-world scenario where labeled attack data may not be available.\n",
    "We explore two learning paradigms:\n",
    "\n",
    "* **Outlier Detection**, where models are trained directly on the mixed dataset (benign + attack) to identify deviations.\n",
    "* **Novelty Detection**, where models are trained only on benign samples and tested on unseen (potentially malicious) data.\n",
    "\n",
    "Two state-of-the-art algorithms are used:\n",
    "\n",
    "* **Isolation Forest (IF)** ‚Äî based on the principle of isolating anomalies using random partitions.\n",
    "* **Local Outlier Factor (LOF)** ‚Äî identifies local density deviations of data points relative to their neighbors.\n",
    "\n",
    "By comparing these two detection strategies, we highlight the trade-offs between supervised classification and unsupervised anomaly detection in developing effective UAV intrusion detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9879ec6",
   "metadata": {},
   "source": [
    "# partie Colin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5da1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87797ef3",
   "metadata": {},
   "source": [
    "# partie Mohamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f65c53",
   "metadata": {},
   "source": [
    "### Data Extraction and Preprocessing Pipeline for the T-ITS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc761322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "csv_path = \"Dataset_T-ITS.csv\"     # your original file\n",
    "output_dir = \"Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Definition of ranges (1-based, inclusive) ===\n",
    "ranges = {\n",
    "    \"Benign\":       {\"Cyber\": (1, 9426),     \"Physical\": (9427, 13717)},\n",
    "    \"DoS Attack\":   {\"Cyber\": (13718, 25389),\"Physical\": (25390, 26363)},\n",
    "    \"Replay Attack\":{\"Cyber\": (26364, 38370),\"Physical\": (38371, 39344)},\n",
    "    \"Evil Twin\":    {\"Cyber\": (39345, 45028),\"Physical\": (45029, 50502)},\n",
    "    \"FDI\":          {\"Cyber\": (50503, 53976),\"Physical\": (53977, 54784)}\n",
    "}\n",
    "\n",
    "# === READ WITHOUT HEADER ===\n",
    "df_raw = pd.read_csv(csv_path, header=None)\n",
    "n = len(df_raw)\n",
    "print(f\"üìÇ File loaded ({n} rows)\\n\")\n",
    "\n",
    "# === HEADER DETECTION FUNCTION BY KEYWORD ===\n",
    "def is_header_line(row):\n",
    "    return row.astype(str).str.contains(\"class\", case=False, na=False).any()\n",
    "\n",
    "# === EXTRACTION OF BLOCKS ACCORDING TO RANGES ===\n",
    "for attack, parts in ranges.items():\n",
    "    for part_name, (start, end) in parts.items():\n",
    "        start_idx = max(0, start - 1)\n",
    "        end_idx = min(n, end)\n",
    "        sub_df = df_raw.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        # Find the header line inside the block\n",
    "        header_idx = sub_df.index[sub_df.apply(is_header_line, axis=1)]\n",
    "        if len(header_idx) == 0:\n",
    "            print(f\"‚ö†Ô∏è No header found in {attack} - {part_name} ({start}-{end}), skipped.\")\n",
    "            continue\n",
    "\n",
    "        header_row = header_idx[0]\n",
    "        header = sub_df.loc[header_row]\n",
    "\n",
    "        # Remove lines before the header\n",
    "        sub_df = sub_df.loc[header_row + 1:]\n",
    "        sub_df.columns = header\n",
    "        sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # === üîπ CLEAN EMPTY COLUMNS ===\n",
    "        # Remove columns that are entirely empty\n",
    "        sub_df = sub_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Remove columns with empty or undefined names\n",
    "        sub_df = sub_df.loc[:, [str(c).strip() not in [\"\", \"Unnamed: 0\", \"Unnamed: 1\", \"nan\"] for c in sub_df.columns]]\n",
    "\n",
    "        # Remove columns where all values are NaN or just empty commas\n",
    "        sub_df = sub_df.loc[:, sub_df.apply(lambda col: not all(str(x).strip() in [\"\", \"nan\"] for x in col), axis=0)]\n",
    "\n",
    "        # Count number of valid features\n",
    "        valid_features = [c for c in sub_df.columns if pd.notna(c) and str(c).strip() != \"\"]\n",
    "        n_features = len(valid_features)\n",
    "\n",
    "        # Save the cleaned block\n",
    "        filename = f\"{attack.replace(' ', '_')}_{part_name}.csv\"\n",
    "        path_out = os.path.join(output_dir, filename)\n",
    "        sub_df.to_csv(path_out, index=False)\n",
    "\n",
    "        # Final display\n",
    "        print(f\"‚úÖ {attack} ({part_name}): {len(sub_df)} rows, {n_features} features\")\n",
    "\n",
    "print(\"\\nüéØ Done: all blocks have been extracted and cleaned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80580bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Begnin_Cyber = pd.read_csv('Data/Benign_Cyber.csv')\n",
    "data_Begnin_Physical = pd.read_csv('Data/Benign_Physical.csv')\n",
    "data_DoS_Cyber = pd.read_csv('Data/DoS_Attack_Cyber.csv')\n",
    "data_DoS_Physical = pd.read_csv('Data/DoS_Attack_Physical.csv')\n",
    "data_Replay_Cyber = pd.read_csv('Data/Replay_Attack_Cyber.csv')\n",
    "data_Replay_Physical = pd.read_csv('Data/Replay_Attack_Physical.csv')\n",
    "data_EvilTwin_Cyber = pd.read_csv('Data/Evil_Twin_Cyber.csv')\n",
    "data_EvilTwin_Physical = pd.read_csv('Data/Evil_Twin_Physical.csv')\n",
    "data_FDI_Cyber = pd.read_csv('Data/FDI_Cyber.csv')\n",
    "data_FDI_Physical = pd.read_csv('Data/FDI_Physical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Begnin_Physical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Begnin_Physical.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a068e48",
   "metadata": {},
   "source": [
    "### Physical Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_flight_distance_temperature_continuous(df, title=\"Courbe continue ‚Äî timestamp_p vs distance\"):\n",
    "    \"\"\"\n",
    "    Trace une ligne continue avec des points color√©s par temp√©rature.\n",
    "    \"\"\"\n",
    "    required_cols = [\"timestamp_p\", \"distance\", \"temperature\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"‚ùå Colonne manquante dans le DataFrame : {col}\")\n",
    "\n",
    "    df_sorted = df.sort_values(by=\"timestamp_p\")\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Ligne grise en arri√®re-plan\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_sorted[\"timestamp_p\"],\n",
    "        y=df_sorted[\"distance\"],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"lightgray\", width=1.5),\n",
    "        name=\"Trajectoire\"\n",
    "    ))\n",
    "\n",
    "    # Points color√©s par temp√©rature\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_sorted[\"timestamp_p\"],\n",
    "        y=df_sorted[\"distance\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            color=df_sorted[\"temperature\"],\n",
    "            colorscale=\"Plasma\",\n",
    "            size=6,\n",
    "            colorbar=dict(title=\"Temp√©rature\"),\n",
    "            showscale=True\n",
    "        ),\n",
    "        name=\"Temp√©rature\"\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"timestamp_p\",\n",
    "        yaxis_title=\"distance\",\n",
    "        template=\"plotly_white\",\n",
    "        width=950,\n",
    "        height=700\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "# Exemple d'utilisation :\n",
    "plot_flight_distance_temperature_continuous(data_Begnin_Physical, \n",
    "    title=\"Courbe continue ‚Äî Donn√©es physiques b√©nignes\")\n",
    "plot_flight_distance_temperature_continuous(data_DoS_Physical, \n",
    "    title=\"Courbe continue ‚Äî Donn√©es physiques DoS Attack\")\n",
    "plot_flight_distance_temperature_continuous(data_Replay_Physical, \n",
    "    title=\"Courbe continue ‚Äî Donn√©es physiques Replay Attack\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fcf66f",
   "metadata": {},
   "source": [
    "### Cyber Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b4eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'frame.len' vs 'timestamp_c' for Cyber Data\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# --- Plot 1 : Benign ---\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(data_Begnin_Cyber['timestamp_c'], data_Begnin_Cyber['frame.len'], color='blue')\n",
    "plt.title('Benign Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('Frame Length')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot 2 : DoS ---\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(data_DoS_Cyber['timestamp_c'], data_DoS_Cyber['frame.len'], color='red')\n",
    "plt.title('DoS Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('Frame Length')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot 3 : Replay ---\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(data_Replay_Cyber['timestamp_c'], data_Replay_Cyber['frame.len'], color='green')\n",
    "plt.title('Replay Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('Frame Length')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 'wlan.seq' vs 'timestamp_c' for Cyber Data\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# --- Plot 1 : Benign ---\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(data_Begnin_Cyber['timestamp_c'], data_Begnin_Cyber['wlan.seq'], color='blue')\n",
    "plt.title('Benign Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('wlan.seq')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot 2 : DoS ---\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(data_DoS_Cyber['timestamp_c'], data_DoS_Cyber['wlan.seq'], color='red')\n",
    "plt.title('DoS Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('wlan.seq')\n",
    "plt.grid(True)\n",
    "\n",
    "# --- Plot 3 : Replay ---\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(data_Replay_Cyber['timestamp_c'], data_Replay_Cyber['wlan.seq'], color='green')\n",
    "plt.title('Replay Cyber')\n",
    "plt.xlabel('Timestamp (c)')\n",
    "plt.ylabel('wlan.seq')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec859647",
   "metadata": {},
   "source": [
    "### Merging Physical Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fdd5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Path to the folder containing the physical files\n",
    "input_dir = \"Data\"       # folder where the CSVs are saved\n",
    "output_file = \"Data/Merged_Physical.csv\"\n",
    "\n",
    "# Exact list of files to merge\n",
    "files_to_merge = [\n",
    "    os.path.join(input_dir, \"Benign_Physical.csv\"),\n",
    "    os.path.join(input_dir, \"DoS_Attack_Physical.csv\"),\n",
    "    os.path.join(input_dir, \"Replay_Attack_Physical.csv\")\n",
    "]\n",
    "\n",
    "# === Check file existence ===\n",
    "for f in files_to_merge:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {f}\")\n",
    "\n",
    "# === Read and clean ===\n",
    "dfs = []\n",
    "for i, f in enumerate(files_to_merge):\n",
    "    df = pd.read_csv(f)\n",
    "    \n",
    "    # Remove any row containing \"class\" or \"timestamp\" in the first column (header residues)\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.contains(\"class|timestamp\", case=False, na=False)]\n",
    "    \n",
    "    # Keep the header only for the first file\n",
    "    if i == 0:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        # Ensure consistent column names\n",
    "        df.columns = dfs[0].columns\n",
    "        dfs.append(df)\n",
    "\n",
    "# === Vertical merge ===\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === Final cleaning ===\n",
    "merged_df = merged_df.dropna(how=\"all\")  # remove empty rows\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# === Save the result ===\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Files successfully merged -> {output_file}\")\n",
    "print(f\"Total: {len(merged_df)} rows, {len(merged_df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7843cc29",
   "metadata": {},
   "source": [
    "### Merging Cyber Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "input_dir = \"Data\"          # folder where your .csv files are located\n",
    "output_file = \"Data/Merged_Cyber.csv\"\n",
    "\n",
    "# === Files to merge ===\n",
    "files_to_merge = [\n",
    "    os.path.join(input_dir, \"Benign_Cyber.csv\"),\n",
    "    os.path.join(input_dir, \"DoS_Attack_Cyber.csv\"),\n",
    "    os.path.join(input_dir, \"Replay_Attack_Cyber.csv\")\n",
    "]\n",
    "\n",
    "# === Check file existence ===\n",
    "for f in files_to_merge:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {f}\")\n",
    "\n",
    "# === Read + Clean ===\n",
    "dfs = []\n",
    "for i, f in enumerate(files_to_merge):\n",
    "    df = pd.read_csv(f)\n",
    "    \n",
    "    # Remove any row containing \"class\" or \"timestamp\" in the first column (repeated header case)\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.contains(\"class|timestamp\", case=False, na=False)]\n",
    "    \n",
    "    # Align columns with the first file\n",
    "    if i == 0:\n",
    "        dfs.append(df)\n",
    "    else:\n",
    "        df.columns = dfs[0].columns  # Ensure same structure\n",
    "        dfs.append(df)\n",
    "\n",
    "# === Vertical merge ===\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "merged_df = merged_df.dropna(how=\"all\")   # Remove empty rows\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# === Save the result ===\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Files successfully merged -> {output_file}\")\n",
    "print(f\"Total: {len(merged_df)} rows, {len(merged_df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158022f4",
   "metadata": {},
   "source": [
    "### Merging Cyber and Physical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_dir = \"Data\"\n",
    "output_file = \"Data/Merged_CP.csv\"\n",
    "\n",
    "physical_file = os.path.join(input_dir, \"Merged_Physical.csv\")\n",
    "cyber_file    = os.path.join(input_dir, \"Merged_Cyber.csv\")\n",
    "\n",
    "# === V√©rification de l'existence des fichiers ===\n",
    "for f in [physical_file, cyber_file]:\n",
    "    if not os.path.exists(f):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {f}\")\n",
    "\n",
    "# === Lecture des fichiers ===\n",
    "df_phys = pd.read_csv(physical_file)\n",
    "df_cyber = pd.read_csv(cyber_file)\n",
    "\n",
    "print(f\"üìÇ Loaded:\")\n",
    "print(f\" - Physical: {df_phys.shape[0]} rows, {df_phys.shape[1]} columns\")\n",
    "print(f\" - Cyber:    {df_cyber.shape[0]} rows, {df_cyber.shape[1]} columns\")\n",
    "\n",
    "# === Renommage des colonnes de timestamp ===\n",
    "if \"timestamp_p\" not in df_phys.columns or \"timestamp_c\" not in df_cyber.columns:\n",
    "    raise KeyError(\"‚ùå Les colonnes 'timestamp_p' ou 'timestamp_c' sont manquantes.\")\n",
    "    \n",
    "df_phys = df_phys.rename(columns={\"timestamp_p\": \"timestamp\"})\n",
    "df_cyber = df_cyber.rename(columns={\"timestamp_c\": \"timestamp\"})\n",
    "\n",
    "# === V√©rifier la pr√©sence de la colonne 'class' ===\n",
    "if \"class\" not in df_phys.columns or \"class\" not in df_cyber.columns:\n",
    "    raise KeyError(\"‚ùå Les deux fichiers doivent contenir une colonne 'class'.\")\n",
    "\n",
    "# === Normalisation des noms de classes ===\n",
    "def normalize_class(c):\n",
    "    c = str(c).strip().lower()\n",
    "    if \"dos\" in c:\n",
    "        return \"dos\"\n",
    "    elif \"replay\" in c:\n",
    "        return \"replay\"\n",
    "    elif \"benign\" in c:\n",
    "        return \"benign\"\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "df_phys[\"class\"] = df_phys[\"class\"].apply(normalize_class)\n",
    "df_cyber[\"class\"] = df_cyber[\"class\"].apply(normalize_class)\n",
    "\n",
    "# === Conserver uniquement les classes communes ===\n",
    "common_classes = set(df_phys[\"class\"]).intersection(set(df_cyber[\"class\"]))\n",
    "df_phys = df_phys[df_phys[\"class\"].isin(common_classes)]\n",
    "df_cyber = df_cyber[df_cyber[\"class\"].isin(common_classes)]\n",
    "\n",
    "print(f\"‚úÖ Classes communes conserv√©es: {common_classes}\")\n",
    "\n",
    "# === Tri par timestamp (important pour merge_asof) ===\n",
    "df_phys = df_phys.sort_values(\"timestamp\")\n",
    "df_cyber = df_cyber.sort_values(\"timestamp\")\n",
    "\n",
    "# === Fusion asynchrone ===\n",
    "df_merged = pd.merge_asof(\n",
    "    df_phys,\n",
    "    df_cyber,\n",
    "    on=\"timestamp\",\n",
    "    by=\"class\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "\n",
    "# === Nettoyage final ===\n",
    "df_merged = df_merged.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "# === Placer la colonne 'class' √† la fin ===\n",
    "if \"class\" in df_merged.columns:\n",
    "    cols = [c for c in df_merged.columns if c != \"class\"] + [\"class\"]\n",
    "    df_merged = df_merged[cols]\n",
    "\n",
    "# === Sauvegarde ===\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "\n",
    "# === R√©sum√© ===\n",
    "print(f\"\\n‚úÖ Fusion termin√©e -> {output_file}\")\n",
    "print(f\"Total: {len(df_merged)} lignes, {len(df_merged.columns)} colonnes\")\n",
    "print(f\"Colonnes : {list(df_merged.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353bce5",
   "metadata": {},
   "source": [
    "### Class Distribution Visualization for Cyber , Physical and Cyber_Physical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16180ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# === LOAD DATASETS ===\n",
    "data_Cyber = pd.read_csv(\"Data/Merged_Cyber.csv\")\n",
    "data_Physical = pd.read_csv(\"Data/Merged_Physical.csv\")\n",
    "data_CP = pd.read_csv(\"Data/Merged_CP.csv\")\n",
    "\n",
    "# === COLUMN VERIFICATION ===\n",
    "for name, df in [(\"Cyber\", data_Cyber), (\"Physical\", data_Physical), (\"Cyber-Physical\", data_CP)]:\n",
    "    if \"class\" not in df.columns:\n",
    "        raise KeyError(f\"‚ùå The 'class' column is missing in {name}\")\n",
    "\n",
    "# === NORMALIZE CLASS LABELS ===\n",
    "def normalize_labels(y):\n",
    "    \"\"\"Standardize class names for consistent plotting.\"\"\"\n",
    "    return (\n",
    "        y.astype(str)\n",
    "         .str.lower()\n",
    "         .replace(r\"\\s+\", \"\", regex=True)\n",
    "         .replace({\n",
    "             \"dosattack\": \"dos\",\n",
    "             \"dos_attack\": \"dos\",\n",
    "             \"dos\": \"dos\",\n",
    "             \"attackdos\": \"dos\",\n",
    "             \"replay\": \"replay\",\n",
    "             \"replayattack\": \"replay\",\n",
    "             \"replay_attack\": \"replay\",\n",
    "             \"attackreplay\": \"replay\",\n",
    "             \"benign\": \"benign\"\n",
    "         })\n",
    "    )\n",
    "\n",
    "data_Cyber[\"class\"] = normalize_labels(data_Cyber[\"class\"])\n",
    "data_Physical[\"class\"] = normalize_labels(data_Physical[\"class\"])\n",
    "data_CP[\"class\"] = normalize_labels(data_CP[\"class\"])\n",
    "\n",
    "# === CLASS COUNTS ===\n",
    "class_counts_cyber = data_Cyber[\"class\"].value_counts()\n",
    "class_counts_physical = data_Physical[\"class\"].value_counts()\n",
    "class_counts_cp = data_CP[\"class\"].value_counts()\n",
    "\n",
    "# === FIXED COLOR MAP (same order for all) ===\n",
    "class_colors = {\n",
    "    \"benign\": \"#66c2a5\",   # greenish\n",
    "    \"dos\": \"#fc8d62\",      # orange\n",
    "    \"replay\": \"#8da0cb\"    # bluish\n",
    "}\n",
    "\n",
    "# === Function to get colors in class order ===\n",
    "def get_colors(labels):\n",
    "    return [class_colors.get(lbl, \"#cccccc\") for lbl in labels]\n",
    "\n",
    "# === CREATE SUBPLOTS (1 row, 3 columns) ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# --- Pie chart: Cyber ---\n",
    "axes[0].pie(\n",
    "    class_counts_cyber,\n",
    "    labels=class_counts_cyber.index,\n",
    "    colors=get_colors(class_counts_cyber.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[0].set_title(\"Class Distribution ‚Äî Cyber Data\")\n",
    "\n",
    "# --- Pie chart: Physical ---\n",
    "axes[1].pie(\n",
    "    class_counts_physical,\n",
    "    labels=class_counts_physical.index,\n",
    "    colors=get_colors(class_counts_physical.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[1].set_title(\"Class Distribution ‚Äî Physical Data\")\n",
    "\n",
    "# --- Pie chart: Cyber-Physical ---\n",
    "axes[2].pie(\n",
    "    class_counts_cp,\n",
    "    labels=class_counts_cp.index,\n",
    "    colors=get_colors(class_counts_cp.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[2].set_title(\"Class Distribution ‚Äî Cyber-Physical Data\")\n",
    "\n",
    "# === GLOBAL LEGEND ===\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label=cls.capitalize(),\n",
    "               markerfacecolor=color, markersize=10)\n",
    "    for cls, color in class_colors.items()\n",
    "]\n",
    "fig.legend(handles=handles, loc='lower center', ncol=3, title=\"Class\")\n",
    "\n",
    "# === ADJUSTMENTS ===\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2498740",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_dir = \"Data\"\n",
    "files = {\n",
    "    \"Physical\": f\"{input_dir}/Merged_Physical.csv\",\n",
    "    \"Cyber\": f\"{input_dir}/Merged_Cyber.csv\",\n",
    "    \"CyberPhysical\": f\"{input_dir}/Merged_CP.csv\"\n",
    "}\n",
    "\n",
    "# === FUNCTION TO NORMALIZE CLASS LABELS ===\n",
    "def normalize_class_labels(y):\n",
    "    \"\"\"\n",
    "    Normalize all class names across datasets:\n",
    "    - lowercase\n",
    "    - remove spaces\n",
    "    - unify naming variants (dos, replay, benign)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        y.astype(str)\n",
    "         .str.lower()\n",
    "         .replace(r\"\\s+\", \"\", regex=True)\n",
    "         .replace({\n",
    "             \"dosattack\": \"dos\",\n",
    "             \"dos_attack\": \"dos\",\n",
    "             \"dos\": \"dos\",\n",
    "             \"attackdos\": \"dos\",\n",
    "             \"replay\": \"replay\",\n",
    "             \"replayattack\": \"replay\",\n",
    "             \"replay_attack\": \"replay\",\n",
    "             \"attackreplay\": \"replay\",\n",
    "             \"benign\": \"benign\"\n",
    "         })\n",
    "    )\n",
    "\n",
    "# === COLUMNS TO DROP PER DATASET TYPE ===\n",
    "drop_map = {\n",
    "    \"Physical\": ['class', 'timestamp_p', 'barometer'],\n",
    "    \"Cyber\": ['class', 'timestamp_c', 'frame.number'],\n",
    "    \"CyberPhysical\": ['class', 'timestamp', 'frame.number', 'barometer']\n",
    "}\n",
    "\n",
    "# === PROCESSING PIPELINE ===\n",
    "processed_data = {}\n",
    "\n",
    "for name, path in files.items():\n",
    "    print(f\"\\nüìÇ Loading {name} dataset ...\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"   ‚Üí {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # Drop irrelevant columns safely\n",
    "    X = df.drop(columns=[c for c in drop_map[name] if c in df.columns], errors='ignore')\n",
    "\n",
    "    # Normalize class labels\n",
    "    y = normalize_class_labels(df['class'])\n",
    "\n",
    "    # Shuffle (break temporal dependence)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "    # Min-Max scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    processed_data[name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_scaled\": X_scaled,\n",
    "        \"scaler\": scaler\n",
    "    }\n",
    "\n",
    "    print(f\"   ‚úÖ Processed: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# === SUMMARY OF NORMALIZED CLASS DISTRIBUTIONS ===\n",
    "print(\"\\n=== üìä Normalized Class Distributions ===\")\n",
    "for name, data in processed_data.items():\n",
    "    counts = data[\"y\"].value_counts().to_dict()\n",
    "    print(f\"{name:15s} ‚Üí {counts}\")\n",
    "\n",
    "# === EXTRACT VARIABLES FOR MODELING ===\n",
    "X_physical, X_physical_scaled, y_physical = (\n",
    "    processed_data[\"Physical\"][\"X\"],\n",
    "    processed_data[\"Physical\"][\"X_scaled\"],\n",
    "    processed_data[\"Physical\"][\"y\"]\n",
    ")\n",
    "\n",
    "X_cyber, X_cyber_scaled, y_cyber = (\n",
    "    processed_data[\"Cyber\"][\"X\"],\n",
    "    processed_data[\"Cyber\"][\"X_scaled\"],\n",
    "    processed_data[\"Cyber\"][\"y\"]\n",
    ")\n",
    "\n",
    "X_cp, X_cp_scaled, y_cp = (\n",
    "    processed_data[\"CyberPhysical\"][\"X\"],\n",
    "    processed_data[\"CyberPhysical\"][\"X_scaled\"],\n",
    "    processed_data[\"CyberPhysical\"][\"y\"]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets successfully loaded and preprocessed!\")\n",
    "print(f\"Physical: X={X_physical.shape}, y={y_physical.shape}\")\n",
    "print(f\"Cyber:    X={X_cyber.shape}, y={y_cyber.shape}\")\n",
    "print(f\"Cyber-Physical: X={X_cp.shape}, y={y_cp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dcfbb",
   "metadata": {},
   "source": [
    "### 1. **Supervised Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06b454",
   "metadata": {},
   "source": [
    "#### Principal Component Ananlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7200d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1Ô∏è‚É£ PCA FIT ===\n",
    "pca_cyber = PCA().fit(X_cyber_scaled)\n",
    "pca_physical = PCA().fit(X_physical_scaled)\n",
    "pca_cp = PCA().fit(X_cp_scaled)\n",
    "\n",
    "# === 2Ô∏è‚É£ Cumulative Explained Variance ===\n",
    "var_cum_cyber = np.cumsum(pca_cyber.explained_variance_ratio_)\n",
    "var_cum_physical = np.cumsum(pca_physical.explained_variance_ratio_)\n",
    "var_cum_cp = np.cumsum(pca_cp.explained_variance_ratio_)\n",
    "\n",
    "# === 3Ô∏è‚É£ Plot cumulative variance (3 side-by-side graphs) ===\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# ---- Plot 1: Cyber Dataset ----\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(var_cum_cyber, marker='o', color='blue')\n",
    "plt.title(\"PCA - Cyber Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "# ---- Plot 2: Physical Dataset ----\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(var_cum_physical, marker='o', color='orange')\n",
    "plt.title(\"PCA - Physical Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "# ---- Plot 3: Cyber-Physical Dataset ----\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(var_cum_cp, marker='o', color='green')\n",
    "plt.title(\"PCA - Cyber-Physical Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 4Ô∏è‚É£ PCA with 2 components ===\n",
    "pca2_cyber = PCA(n_components=2)\n",
    "X_cyber_pca2 = pca2_cyber.fit_transform(X_cyber_scaled)\n",
    "\n",
    "pca2_physical = PCA(n_components=2)\n",
    "X_physical_pca2 = pca2_physical.fit_transform(X_physical_scaled)\n",
    "\n",
    "pca2_cp = PCA(n_components=2)\n",
    "X_cp_pca2 = pca2_cp.fit_transform(X_cp_scaled)\n",
    "\n",
    "# === 5Ô∏è‚É£ Define classes and colors ===\n",
    "colors = ['r', 'b', 'g', 'purple', 'orange']\n",
    "\n",
    "classes_cyber = np.unique(y_cyber)\n",
    "classes_physical = np.unique(y_physical)\n",
    "classes_cp = np.unique(y_cp)\n",
    "\n",
    "# === 6Ô∏è‚É£ 2D PCA Visualization (3 side-by-side graphs) ===\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# ---- PCA Cyber ----\n",
    "plt.subplot(1, 3, 1)\n",
    "for i, cls in enumerate(classes_cyber):\n",
    "    plt.scatter(\n",
    "        X_cyber_pca2[y_cyber == cls, 0],\n",
    "        X_cyber_pca2[y_cyber == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Cyber Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- PCA Physical ----\n",
    "plt.subplot(1, 3, 2)\n",
    "for i, cls in enumerate(classes_physical):\n",
    "    plt.scatter(\n",
    "        X_physical_pca2[y_physical == cls, 0],\n",
    "        X_physical_pca2[y_physical == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Physical Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- PCA Cyber-Physical ----\n",
    "plt.subplot(1, 3, 3)\n",
    "for i, cls in enumerate(classes_cp):\n",
    "    plt.scatter(\n",
    "        X_cp_pca2[y_cp == cls, 0],\n",
    "        X_cp_pca2[y_cp == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Cyber-Physical Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcb21c",
   "metadata": {},
   "source": [
    "#### SVM, Kernel Trick and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Parameter grids for each kernel ===\n",
    "param_grids = {\n",
    "    \"linear\": {\n",
    "        \"C\": [0.1, 1, 10]\n",
    "    },\n",
    "    \"poly\": {\n",
    "        \"C\": [0.1, 1],\n",
    "        \"degree\": [2, 3, 4, 5],\n",
    "    },\n",
    "    \"rbf\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [1e-2, 1e-1, 1, 10]\n",
    "    },\n",
    "    \"sigmoid\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"coef0\": [0, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Store results ===\n",
    "results = {\"cyber\": {}, \"physical\": {}, \"cyber_physical\": {}}\n",
    "\n",
    "# === Loop through all three datasets ===\n",
    "for dataset in [\"cyber\", \"physical\", \"cyber_physical\"]:\n",
    "\n",
    "    print(f\"\\n\\n==================== üß© DATASET: {dataset.upper()} ====================\")\n",
    "\n",
    "    # --- Dataset configuration ---\n",
    "    if dataset == \"cyber\":\n",
    "        n_half = len(X_cyber_scaled) // 4\n",
    "        X_scaled = X_cyber_scaled[:n_half]\n",
    "        y = y_cyber[:n_half]\n",
    "        n_comp = 4\n",
    "        print(f\"üìâ Cyber sub-sampling: keeping {n_half} rows\")\n",
    "    elif dataset == \"physical\":\n",
    "        X_scaled = X_physical_scaled\n",
    "        y = y_physical\n",
    "        n_comp = 5\n",
    "    else:  # cyber_physical\n",
    "        X_scaled = X_cp_scaled\n",
    "        y = y_cp\n",
    "        n_comp = 8\n",
    "\n",
    "    # --- PCA dimensionality reduction ---\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    print(f\"üîπ {dataset.capitalize()}: variance explained by {n_comp} components = {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "    # --- Train/test split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Loop over kernel types ---\n",
    "    for kernel_choice, param_grid in param_grids.items():\n",
    "        print(f\"\\n=== ‚öôÔ∏è GridSearch for kernel = '{kernel_choice}' ===\")\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            SVC(kernel=kernel_choice, decision_function_shape='ovr'),\n",
    "            param_grid=param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_score = grid.best_score_\n",
    "        best_params = grid.best_params_\n",
    "\n",
    "        print(f\"‚úÖ Best parameters: {best_params}\")\n",
    "        print(f\"üèÅ Cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "        # --- Final training ---\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"üéØ Test accuracy: {acc_test:.4f}\")\n",
    "        results[dataset][kernel_choice] = acc_test\n",
    "\n",
    "# === Summary of results ===\n",
    "print(\"\\n==================== üèÅ SUMMARY OF TEST ACCURACIES ====================\")\n",
    "for ds, scores in results.items():\n",
    "    print(f\"\\nüìä {ds.upper()}:\")\n",
    "    for k, s in scores.items():\n",
    "        print(f\"  {k:10s} : {s:.4f}\")\n",
    "\n",
    "# === Visualization ===\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "kernels = list(param_grids.keys())\n",
    "x = np.arange(len(kernels))\n",
    "width = 0.25\n",
    "\n",
    "cyber_scores = [results[\"cyber\"].get(k, 0) for k in kernels]\n",
    "phys_scores = [results[\"physical\"].get(k, 0) for k in kernels]\n",
    "cp_scores = [results[\"cyber_physical\"].get(k, 0) for k in kernels]\n",
    "\n",
    "bars1 = ax.bar(x - width, cyber_scores, width, label='Cyber', alpha=0.8)\n",
    "bars2 = ax.bar(x, phys_scores, width, label='Physical', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, cp_scores, width, label='Cyber-Physical', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Kernel type\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.set_title(\"SVM Performance Comparison ‚Äî Cyber vs Physical vs Cyber-Physical\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(kernels)\n",
    "ax.legend()\n",
    "ax.bar_label(bars1, fmt='%.2f', padding=3)\n",
    "ax.bar_label(bars2, fmt='%.2f', padding=3)\n",
    "ax.bar_label(bars3, fmt='%.2f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98890d",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae88523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "datasets = {\n",
    "    \"Cyber\": (X_cyber, y_cyber),\n",
    "    \"Physical\": (X_physical, y_physical),\n",
    "    \"Cyber-Physical\": (X_cp, y_cp)\n",
    "}\n",
    "\n",
    "n_estimators = 200\n",
    "random_state = 42\n",
    "k_folds = 5\n",
    "\n",
    "# === STORE RESULTS ===\n",
    "per_class_metrics = []\n",
    "kfold_results = {}\n",
    "\n",
    "# === LOOP OVER DATASETS ===\n",
    "for name, (X, y) in datasets.items():\n",
    "    X, y = shuffle(X, y, random_state=random_state)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.25, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion='entropy', random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    for cls in report.keys():\n",
    "        if cls in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "            continue\n",
    "        per_class_metrics.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Class\": cls.lower(),\n",
    "            \"Accuracy\": report[cls][\"precision\"],\n",
    "            \"Recall\": report[cls][\"recall\"],\n",
    "            \"F1\": report[cls][\"f1-score\"]\n",
    "        })\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    scores = cross_val_score(rf, X_scaled, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    kfold_results[name] = scores\n",
    "\n",
    "df_class_metrics = pd.DataFrame(per_class_metrics)\n",
    "\n",
    "# === COLORS BY CLASS ===\n",
    "class_colors = {\n",
    "    \"benign\": \"#66c2a5\",\n",
    "    \"dos\": \"#fc8d62\",\n",
    "    \"replay\": \"#8da0cb\"\n",
    "}\n",
    "\n",
    "# === PLOTS FOR PER-CLASS METRICS SIDE BY SIDE ===\n",
    "metrics = [(\"Accuracy\", \"Accuracy (Precision)\"),\n",
    "           (\"Recall\", \"Recall\"),\n",
    "           (\"F1\", \"F1-score\")]\n",
    "\n",
    "datasets_order = [\"Cyber\", \"Physical\", \"Cyber-Physical\"]\n",
    "class_list = list(class_colors.keys())\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(datasets_order))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "\n",
    "for ax, (metric_name, ylabel) in zip(axes, metrics):\n",
    "    for i, cls in enumerate(class_list):\n",
    "        subset = df_class_metrics[df_class_metrics[\"Class\"].str.lower() == cls]\n",
    "        subset = subset.set_index(\"Dataset\").reindex(datasets_order)\n",
    "        ax.bar(\n",
    "            x + i * bar_width,\n",
    "            subset[metric_name],\n",
    "            width=bar_width,\n",
    "            label=cls.capitalize() if metric_name == \"Accuracy\" else \"\",\n",
    "            color=class_colors[cls]\n",
    "        )\n",
    "        for j, val in enumerate(subset[metric_name]):\n",
    "            if not np.isnan(val):\n",
    "                ax.text(x[j] + i * bar_width, val + 0.01, f\"{val:.2f}\", ha='center', fontsize=9)\n",
    "\n",
    "    ax.set_xticks(x + bar_width)\n",
    "    ax.set_xticklabels(datasets_order)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(metric_name)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "fig.legend(title=\"Class\", loc='upper center', ncol=len(class_list))\n",
    "fig.suptitle(\"Random Forest ‚Äî Performance Metrics by Class and Dataset\", fontsize=13, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === K-FOLD CROSS-VALIDATION SUMMARY (text only) ===\n",
    "print(\"\\n=== üèÅ Random Forest ‚Äî K-Fold Cross-Validation Summary ===\")\n",
    "for name in datasets_order:\n",
    "    mean_acc = np.mean(kfold_results[name])\n",
    "    std_acc = np.std(kfold_results[name])\n",
    "    print(f\"{name:<15s} ‚Üí Mean Accuracy: {mean_acc:.4f} ¬± {std_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f2d8",
   "metadata": {},
   "source": [
    "### 2. **Unsupervised Anomaly Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "#         IMPORTS\n",
    "# ================================\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import warnings\n",
    "# ================================\n",
    "#        COMMON FUNCTION\n",
    "# ================================\n",
    "import warnings\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_true, mode, name, contamination):\n",
    "    \"\"\"Fit/predict + compute F1, precision, recall and AUC for outlier or novelty mode.\"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "        if \"Local Outlier Factor\" in name:\n",
    "            # Just increase neighbors, don't remove duplicates\n",
    "            n_neighbors = min(25, len(X_train) - 1)\n",
    "\n",
    "            if mode == \"outlier\":\n",
    "                lof = LocalOutlierFactor(\n",
    "                    n_neighbors=n_neighbors,\n",
    "                    contamination=contamination,\n",
    "                    novelty=False\n",
    "                )\n",
    "                y_pred = lof.fit_predict(X_test)\n",
    "            else:\n",
    "                lof = LocalOutlierFactor(\n",
    "                    n_neighbors=n_neighbors,\n",
    "                    contamination=contamination,\n",
    "                    novelty=True\n",
    "                )\n",
    "                lof.fit(X_train)\n",
    "                y_pred = lof.predict(X_test)\n",
    "        else:\n",
    "            if mode == \"outlier\":\n",
    "                model.fit(X_test)\n",
    "                y_pred = model.predict(X_test)\n",
    "            else:\n",
    "                model.fit(X_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "    # === Keep consistent lengths ===\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(f\"Length mismatch: y_true={len(y_true)}, y_pred={len(y_pred)}\")\n",
    "\n",
    "    y_pred = np.where(y_pred == -1, -1, 1)\n",
    "    report = classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=['Attack (-1)', 'Benign (+1)'],\n",
    "        output_dict=True\n",
    "    )\n",
    "\n",
    "    f1 = report['Attack (-1)']['f1-score']\n",
    "    precision = report['Attack (-1)']['precision']\n",
    "    recall = report['Attack (-1)']['recall']\n",
    "    auc = roc_auc_score((y_true == 1).astype(int), (y_pred == 1).astype(int))\n",
    "\n",
    "    return f1, precision, recall, auc\n",
    "\n",
    "\n",
    "def run_experiment(X, y, dataset_name, contamination):\n",
    "    \"\"\"Run anomaly detection on unscaled data.\"\"\"\n",
    "    print(f\"\\n=== {dataset_name.upper()} DATASET (contamination={contamination}) ===\")\n",
    "\n",
    "    mask_benign = (y == 'benign')\n",
    "    X_train = X[mask_benign]\n",
    "    X_test = X\n",
    "    y_true = np.where(y == 'benign', 1, -1)\n",
    "\n",
    "    print(f\"Training on {len(X_train)} benign samples\")\n",
    "\n",
    "    base_models = {\n",
    "        \"Isolation Forest\": IsolationForest(contamination=contamination, random_state=42),\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, base_model in base_models.items():\n",
    "        print(f\"\\nüîπ {name}\")\n",
    "        model_outlier = copy.deepcopy(base_model)\n",
    "        model_novelty = copy.deepcopy(base_model)\n",
    "\n",
    "        f1_outlier, prec_outlier, rec_outlier, _ = evaluate_model(model_outlier, X_train, X_test, y_true, 'outlier', name, contamination)\n",
    "        f1_novelty, prec_novelty, rec_novelty, _ = evaluate_model(model_novelty, X_train, X_test, y_true, 'novelty', name, contamination)\n",
    "\n",
    "        results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Model\": name,\n",
    "            \"Mode\": \"Outlier Detection\",\n",
    "            \"F1-score\": f1_outlier,\n",
    "            \"Precision\": prec_outlier,\n",
    "            \"Recall\": rec_outlier\n",
    "        })\n",
    "        results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Model\": name,\n",
    "            \"Mode\": \"Novelty Detection\",\n",
    "            \"F1-score\": f1_novelty,\n",
    "            \"Precision\": prec_novelty,\n",
    "            \"Recall\": rec_novelty\n",
    "        })\n",
    "\n",
    "    # === Add Local Outlier Factor explicitly ===\n",
    "    print(\"\\nüîπ Local Outlier Factor\")\n",
    "    for mode in ['outlier', 'novelty']:\n",
    "        f1, prec, rec, _ = evaluate_model(None, X_train, X_test, y_true, mode, \"Local Outlier Factor\", contamination)\n",
    "        results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Model\": \"Local Outlier Factor\",\n",
    "            \"Mode\": \"Outlier Detection\" if mode == \"outlier\" else \"Novelty Detection\",\n",
    "            \"F1-score\": f1,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ================================\n",
    "#       RUN ALL 3 DATASETS\n",
    "# ================================\n",
    "df_cyber = run_experiment(X_cyber, y_cyber, \"Cyber\", contamination=0.5)\n",
    "df_physical = run_experiment(X_physical, y_physical, \"Physical\", contamination=0.32)\n",
    "df_cp = run_experiment(X_cp, y_cp, \"Cyber-Physical\", contamination=0.32)\n",
    "\n",
    "df_all = pd.concat([df_cyber, df_physical, df_cp], ignore_index=True)\n",
    "\n",
    "# === Clean labels ===\n",
    "df_all.rename(columns={\n",
    "    \"Dataset\": \"Data\",\n",
    "    \"Model\": \"Algo\",\n",
    "    \"Mode\": \"Type\",\n",
    "    \"F1-score\": \"F1\",\n",
    "    \"Precision\": \"Prec\",\n",
    "    \"Recall\": \"Rec\"\n",
    "}, inplace=True)\n",
    "\n",
    "df_all[\"Algo\"] = df_all[\"Algo\"].replace({\n",
    "    \"Isolation Forest\": \"IF\",\n",
    "    \"Local Outlier Factor\": \"LOF\"\n",
    "})\n",
    "df_all[\"Type\"] = df_all[\"Type\"].replace({\n",
    "    \"Outlier Detection\": \"Outlier\",\n",
    "    \"Novelty Detection\": \"Novelty\"\n",
    "})\n",
    "\n",
    "df_all = df_all[[\"Data\", \"Algo\", \"Type\", \"F1\", \"Prec\", \"Rec\"]].round(3)\n",
    "print(\"\\n‚úÖ Combined Results (real, unscaled data):\")\n",
    "print(df_all.to_string(index=False))\n",
    "\n",
    "# ================================\n",
    "#   2√ó3 COMPARATIVE VISUALIZATION\n",
    "# ================================\n",
    "metrics = [ \"Prec\", \"Rec\",\"F1\"]\n",
    "types = [\"Outlier\", \"Novelty\"]\n",
    "datasets = [\"Cyber\", \"Physical\", \"Cyber-Physical\"]\n",
    "algos = df_all[\"Algo\"].unique()\n",
    "colors = {\"Cyber\": \"#66c2a5\", \"Physical\": \"#fc8d62\", \"Cyber-Physical\": \"#8da0cb\"}\n",
    "width = 0.25\n",
    "x = np.arange(len(algos))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "fig.suptitle(\"Anomaly Detection Comparison (Unscaled Data) ‚Äî Outlier vs Novelty\", fontsize=16, weight=\"bold\")\n",
    "\n",
    "for i, det_type in enumerate(types):\n",
    "    for j, metric in enumerate(metrics):\n",
    "        ax = axes[i, j]\n",
    "        for k, dataset in enumerate(datasets):\n",
    "            subset = df_all[\n",
    "                (df_all[\"Type\"] == det_type) &\n",
    "                (df_all[\"Data\"] == dataset)\n",
    "            ]\n",
    "            subset = subset.set_index(\"Algo\").reindex(algos)\n",
    "            ax.bar(\n",
    "                x + k * width,\n",
    "                subset[metric],\n",
    "                width=width,\n",
    "                label=dataset if j == 0 else \"\",\n",
    "                color=colors[dataset]\n",
    "            )\n",
    "            # Add value labels\n",
    "            for m, val in enumerate(subset[metric]):\n",
    "                if not np.isnan(val):\n",
    "                    ax.text(x[m] + k * width, val + 0.01, f\"{val:.2f}\", ha='center', fontsize=8)\n",
    "\n",
    "        ax.set_title(f\"{det_type} ‚Äî {metric}\", fontsize=12)\n",
    "        ax.set_xticks(x + width)\n",
    "        ax.set_xticklabels(algos)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(\"Score\")\n",
    "\n",
    "# Shared legend (visible and centered below plots)\n",
    "# === Shared legend (collect from all axes) ===\n",
    "handles, labels = [], []\n",
    "for ax in axes.ravel():\n",
    "    h, l = ax.get_legend_handles_labels()\n",
    "    for hh, ll in zip(h, l):\n",
    "        if ll not in labels:  # avoid duplicates\n",
    "            handles.append(hh)\n",
    "            labels.append(ll)\n",
    "\n",
    "fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='lower center',\n",
    "    ncol=3,\n",
    "    bbox_to_anchor=(0.5, -0.02),\n",
    "    fontsize=10,\n",
    "    title=\"Dataset\"\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
