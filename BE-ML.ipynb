{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9632e0c3",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">BE - ML</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82150020",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Unmanned Aerial Vehicles (UAVs), commonly known as drones, are increasingly being deployed for applications such as surveillance, delivery, environmental monitoring, and disaster management. However, the growing reliance on UAVs also exposes them to significant **cybersecurity risks**, as malicious actors can exploit vulnerabilities in their communication and control systems. To address this challenge, **intrusion detection systems (IDSs)** leveraging machine learning have become an active area of research.\n",
    "\n",
    "This project utilizes the **Cyber-Physical Dataset for UAVs Under Normal Operations and Cyberattacks**, developed by **Hassler, Mughal, and Ismail (2023)**. The dataset captures both **cyber and physical telemetry data** from UAVs under normal flight conditions and during four distinct types of cyberattacks:\n",
    "\n",
    "1. **De-authentication Denial-of-Service (DoS) attack**\n",
    "2. **Replay attack**\n",
    "3. **False Data Injection (FDI) attack**\n",
    "4. **Evil Twin attack**\n",
    "\n",
    "The dataset is composed of two major components:\n",
    "\n",
    "* **Cyber dataset** containing 37 features that describe network-level parameters and communication behavior.\n",
    "* **Physical dataset** containing 16 features that represent UAV motion dynamics such as altitude, velocity, position, and orientation.\n",
    "\n",
    "Each record in the dataset corresponds to either **benign (normal)** operation or one of the aforementioned **attack scenarios**, making it suitable for **binary (normal vs. malicious)** or **multi-class** classification tasks. The data were collected using a custom UAV testbed with real-time attack emulation and monitoring tools.\n",
    "\n",
    "\n",
    "**Reference:**\n",
    "Hassler, S. C., Mughal, U. A., & Ismail, M. (2023). *Cyber-Physical Intrusion Detection System for Unmanned Aerial Vehicles*. IEEE Transactions on Intelligent Transportation Systems.\n",
    "Available via GitHub: [https://github.com/uamughal/UAVs-Dataset-Under-Normal-and-Cyberattacks](https://github.com/uamughal/UAVs-Dataset-Under-Normal-and-Cyberattacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e82d1",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "\n",
    "In this notebook, we aim to design and evaluate intelligent **cyber-physical intrusion detection systems (IDS)** capable of detecting abnormal UAV behaviors in real time.\n",
    "To achieve this, the problem is approached in **two complementary ways**:\n",
    "\n",
    "### 1. **Supervised Classification**\n",
    "\n",
    "In this first stage, the task is treated as a standard **supervised learning problem** where each observation is labeled as *benign* or *malicious*.\n",
    "We train and compare two widely used machine learning algorithms:\n",
    "\n",
    "* **Support Vector Machine (SVM)** ‚Äî a robust classifier that finds optimal separating hyperplanes.\n",
    "* **Random Forest (RF)** ‚Äî an ensemble method that combines multiple decision trees to improve generalization and interpretability.\n",
    "\n",
    "These models are evaluated in terms of precision, recall, and F1-score across different datasets (Cyber, Physical, and Cyber-Physical).\n",
    "\n",
    "### 2. **Unsupervised Anomaly Detection**\n",
    "\n",
    "In the second stage, the problem is reformulated as an **anomaly detection task**, simulating a real-world scenario where labeled attack data may not be available.\n",
    "We explore two learning paradigms:\n",
    "\n",
    "* **Outlier Detection**, where models are trained directly on the mixed dataset (benign + attack) to identify deviations.\n",
    "* **Novelty Detection**, where models are trained only on benign samples and tested on unseen (potentially malicious) data.\n",
    "\n",
    "Two state-of-the-art algorithms are used:\n",
    "\n",
    "* **Isolation Forest (IF)** ‚Äî based on the principle of isolating anomalies using random partitions.\n",
    "* **Local Outlier Factor (LOF)** ‚Äî identifies local density deviations of data points relative to their neighbors.\n",
    "\n",
    "By comparing these two detection strategies, we highlight the trade-offs between supervised classification and unsupervised anomaly detection in developing effective UAV intrusion detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9879ec6",
   "metadata": {},
   "source": [
    "# partie Colin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5da1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87797ef3",
   "metadata": {},
   "source": [
    "# partie Mohamed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545912f",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f65c53",
   "metadata": {},
   "source": [
    "### **Data acquisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef7094",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0c30e",
   "metadata": {},
   "source": [
    "In this step, we extract and clean the raw UAV dataset from the original combined CSV file. Each attack type and data modality (Cyber or Physical) is separated into individual CSV files based on predefined row ranges. During this process, we detect and apply proper headers, remove empty or irrelevant columns, and save the cleaned subsets to the output directory for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc761322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "csv_path = \"Dataset_T-ITS.csv\"     # your original file\n",
    "output_dir = \"Data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Definition of ranges (1-based, inclusive) ===\n",
    "ranges = {\n",
    "    \"Benign\":       {\"Cyber\": (1, 9426),     \"Physical\": (9427, 13717)},\n",
    "    \"DoS Attack\":   {\"Cyber\": (13718, 25389),\"Physical\": (25390, 26363)},\n",
    "    \"Replay Attack\":{\"Cyber\": (26364, 38370),\"Physical\": (38371, 39344)},\n",
    "    \"Evil Twin\":    {\"Cyber\": (39345, 45028),\"Physical\": (45029, 50502)},\n",
    "    \"FDI\":          {\"Cyber\": (50503, 53976),\"Physical\": (53977, 54784)}\n",
    "}\n",
    "\n",
    "# === READ WITHOUT HEADER ===\n",
    "df_raw = pd.read_csv(csv_path, header=None)\n",
    "n = len(df_raw)\n",
    "print(f\"üìÇ File loaded ({n} rows)\\n\")\n",
    "\n",
    "# === HEADER DETECTION FUNCTION BY KEYWORD ===\n",
    "def is_header_line(row):\n",
    "    return row.astype(str).str.contains(\"class\", case=False, na=False).any()\n",
    "\n",
    "# === EXTRACTION OF BLOCKS ACCORDING TO RANGES ===\n",
    "for attack, parts in ranges.items():\n",
    "    for part_name, (start, end) in parts.items():\n",
    "        start_idx = max(0, start - 1)\n",
    "        end_idx = min(n, end)\n",
    "        sub_df = df_raw.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "        # Find the header line inside the block\n",
    "        header_idx = sub_df.index[sub_df.apply(is_header_line, axis=1)]\n",
    "        if len(header_idx) == 0:\n",
    "            print(f\"‚ö†Ô∏è No header found in {attack} - {part_name} ({start}-{end}), skipped.\")\n",
    "            continue\n",
    "\n",
    "        header_row = header_idx[0]\n",
    "        header = sub_df.loc[header_row]\n",
    "\n",
    "        # Remove lines before the header\n",
    "        sub_df = sub_df.loc[header_row + 1:]\n",
    "        sub_df.columns = header\n",
    "        sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # === üîπ CLEAN EMPTY COLUMNS ===\n",
    "        # Remove columns that are entirely empty\n",
    "        sub_df = sub_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Remove columns with empty or undefined names\n",
    "        sub_df = sub_df.loc[:, [str(c).strip() not in [\"\", \"Unnamed: 0\", \"Unnamed: 1\", \"nan\"] for c in sub_df.columns]]\n",
    "\n",
    "        # Remove columns where all values are NaN or just empty commas\n",
    "        sub_df = sub_df.loc[:, sub_df.apply(lambda col: not all(str(x).strip() in [\"\", \"nan\"] for x in col), axis=0)]\n",
    "\n",
    "        # Count number of valid features\n",
    "        valid_features = [c for c in sub_df.columns if pd.notna(c) and str(c).strip() != \"\"]\n",
    "        n_features = len(valid_features)\n",
    "\n",
    "        # Save the cleaned block\n",
    "        filename = f\"{attack.replace(' ', '_')}_{part_name}.csv\"\n",
    "        path_out = os.path.join(output_dir, filename)\n",
    "        sub_df.to_csv(path_out, index=False)\n",
    "\n",
    "        # Final display\n",
    "        print(f\"‚úÖ {attack} ({part_name}): {len(sub_df)} rows, {n_features} features\")\n",
    "\n",
    "print(\"\\nüéØ Done: all blocks have been extracted and cleaned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2dbf67",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "As observed, the *Evil Twin* and *FDI* attack datasets (for both physical and cyber domains) contain a different number of features compared to the other classes, and their feature names also vary. Upon reviewing the original GitHub repository, several users noted that these subsets likely originate from a different research study. To ensure homogeneity and consistency across the dataset, we decided to **exclude the Evil Twin and FDI attacks (both physical and cyber)** from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56307f",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f83712",
   "metadata": {},
   "source": [
    "The merging process combines the physical and cyber datasets based on synchronized timestamps and matching class labels, ensuring a unified cyber-physical dataset suitable for integrated UAV system analysis. As a result, we will create three datasets ‚Äî Physical, Cyber, and Cyber-Physical ‚Äî each containing three classes: Benign, DoS Attack, and Replay Attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#            UAVs Dataset Merge Utility\n",
    "# =====================================================\n",
    "# Combines:\n",
    "#   1Ô∏è‚É£ Physical data files\n",
    "#   2Ô∏è‚É£ Cyber data files\n",
    "#   3Ô∏è‚É£ Merges both into Cyber-Physical dataset\n",
    "# =====================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIGURATION ----------------\n",
    "DATA_DIR = \"Data\"\n",
    "OUTPUTS = {\n",
    "    \"physical\": os.path.join(DATA_DIR, \"Merged_Physical.csv\"),\n",
    "    \"cyber\": os.path.join(DATA_DIR, \"Merged_Cyber.csv\"),\n",
    "    \"cp\": os.path.join(DATA_DIR, \"Merged_CP.csv\"),\n",
    "}\n",
    "\n",
    "PHYSICAL_FILES = [\n",
    "    \"Benign_Physical.csv\",\n",
    "    \"DoS_Attack_Physical.csv\",\n",
    "    \"Replay_Attack_Physical.csv\",\n",
    "]\n",
    "\n",
    "CYBER_FILES = [\n",
    "    \"Benign_Cyber.csv\",\n",
    "    \"DoS_Attack_Cyber.csv\",\n",
    "    \"Replay_Attack_Cyber.csv\",\n",
    "]\n",
    "\n",
    "# ---------------- UTILITIES ----------------\n",
    "def check_files_exist(files):\n",
    "    \"\"\"Ensure all files exist before processing.\"\"\"\n",
    "    for f in files:\n",
    "        if not os.path.exists(f):\n",
    "            raise FileNotFoundError(f\"‚ùå File not found: {f}\")\n",
    "\n",
    "def clean_csv(df):\n",
    "    \"\"\"Remove repeated headers and empty rows.\"\"\"\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.contains(\"class|timestamp\", case=False, na=False)]\n",
    "    return df.dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "def merge_csv_files(file_list, output_path):\n",
    "    \"\"\"Read, clean, and vertically concatenate multiple CSVs.\"\"\"\n",
    "    check_files_exist(file_list)\n",
    "\n",
    "    dfs = []\n",
    "    for i, f in enumerate(file_list):\n",
    "        df = pd.read_csv(f)\n",
    "        df = clean_csv(df)\n",
    "        if i > 0:\n",
    "            df.columns = dfs[0].columns  # align column names\n",
    "        dfs.append(df)\n",
    "\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Merged {len(file_list)} files -> {output_path}\")\n",
    "    print(f\"   {len(merged_df)} rows, {len(merged_df.columns)} columns\\n\")\n",
    "    return merged_df\n",
    "\n",
    "def normalize_class(c):\n",
    "    \"\"\"Standardize class labels.\"\"\"\n",
    "    c = str(c).strip().lower()\n",
    "    if \"dos\" in c:\n",
    "        return \"dos\"\n",
    "    elif \"replay\" in c:\n",
    "        return \"replay\"\n",
    "    elif \"benign\" in c:\n",
    "        return \"benign\"\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "def merge_cyber_physical(df_phys, df_cyber, output_path):\n",
    "    \"\"\"Synchronize and merge physical and cyber datasets.\"\"\"\n",
    "    # Check timestamps\n",
    "    if \"timestamp_p\" not in df_phys.columns or \"timestamp_c\" not in df_cyber.columns:\n",
    "        raise KeyError(\"‚ùå Columns 'timestamp_p' or 'timestamp_c' are missing.\")\n",
    "\n",
    "    # Rename for alignment\n",
    "    df_phys = df_phys.rename(columns={\"timestamp_p\": \"timestamp\"})\n",
    "    df_cyber = df_cyber.rename(columns={\"timestamp_c\": \"timestamp\"})\n",
    "\n",
    "    # Check 'class' column\n",
    "    if \"class\" not in df_phys.columns or \"class\" not in df_cyber.columns:\n",
    "        raise KeyError(\"‚ùå Both files must contain a 'class' column.\")\n",
    "\n",
    "    # Normalize classes\n",
    "    df_phys[\"class\"] = df_phys[\"class\"].apply(normalize_class)\n",
    "    df_cyber[\"class\"] = df_cyber[\"class\"].apply(normalize_class)\n",
    "\n",
    "    # Keep only common classes\n",
    "    common_classes = set(df_phys[\"class\"]) & set(df_cyber[\"class\"])\n",
    "    df_phys = df_phys[df_phys[\"class\"].isin(common_classes)]\n",
    "    df_cyber = df_cyber[df_cyber[\"class\"].isin(common_classes)]\n",
    "    print(f\"‚úÖ Common classes: {common_classes}\")\n",
    "\n",
    "    # Sort and merge\n",
    "    df_phys = df_phys.sort_values(\"timestamp\")\n",
    "    df_cyber = df_cyber.sort_values(\"timestamp\")\n",
    "\n",
    "    df_merged = pd.merge_asof(\n",
    "        df_phys,\n",
    "        df_cyber,\n",
    "        on=\"timestamp\",\n",
    "        by=\"class\",\n",
    "        direction=\"backward\"\n",
    "    ).dropna(how=\"all\").reset_index(drop=True)\n",
    "\n",
    "    # Move 'class' to the end\n",
    "    if \"class\" in df_merged.columns:\n",
    "        cols = [c for c in df_merged.columns if c != \"class\"] + [\"class\"]\n",
    "        df_merged = df_merged[cols]\n",
    "\n",
    "    df_merged.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Cyber-Physical merged -> {output_path}\")\n",
    "    print(f\"   {len(df_merged)} rows, {len(df_merged.columns)} columns\\n\")\n",
    "    return df_merged\n",
    "\n",
    "# ---------------- MAIN PIPELINE ----------------\n",
    "def main():\n",
    "    print(\"üöÄ Starting UAV Dataset Merge Pipeline\\n\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Merge Physical\n",
    "    phys_paths = [os.path.join(DATA_DIR, f) for f in PHYSICAL_FILES]\n",
    "    df_phys = merge_csv_files(phys_paths, OUTPUTS[\"physical\"])\n",
    "\n",
    "    # 2Ô∏è‚É£ Merge Cyber\n",
    "    cyber_paths = [os.path.join(DATA_DIR, f) for f in CYBER_FILES]\n",
    "    df_cyber = merge_csv_files(cyber_paths, OUTPUTS[\"cyber\"])\n",
    "\n",
    "    # 3Ô∏è‚É£ Merge Cyber + Physical\n",
    "    df_phys = pd.read_csv(OUTPUTS[\"physical\"])\n",
    "    df_cyber = pd.read_csv(OUTPUTS[\"cyber\"])\n",
    "    merge_cyber_physical(df_phys, df_cyber, OUTPUTS[\"cp\"])\n",
    "\n",
    "    print(\"üéØ All merges completed successfully.\")\n",
    "\n",
    "# ---------------- EXECUTION ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a068e48",
   "metadata": {},
   "source": [
    "### **Data exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b861e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOAD DATASETS ===\n",
    "data_Cyber = pd.read_csv(\"Data/Merged_Cyber.csv\")\n",
    "data_Physical = pd.read_csv(\"Data/Merged_Physical.csv\")\n",
    "data_CP = pd.read_csv(\"Data/Merged_CP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e87bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Cyber.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3db73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Physical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804d94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CP.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa2734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#          UAVs Dataset ‚Äî Exploratory Visualization\n",
    "# =====================================================\n",
    "# Plots the top 2 most important features for each dataset:\n",
    "#   - Cyber:      time_since_last_packet vs wlan.seq\n",
    "#   - Physical:   battery vs temperature\n",
    "#   - Cyber-Phys: battery (physical) vs wlan.seq (cyber)\n",
    "# =====================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# ---------------- CONFIGURATION ----------------\n",
    "DATA_DIR = \"Data\"\n",
    "DATASETS = {\n",
    "    \"Cyber\": os.path.join(DATA_DIR, \"Merged_Cyber.csv\"),\n",
    "    \"Physical\": os.path.join(DATA_DIR, \"Merged_Physical.csv\"),\n",
    "    \"Cyber-Physical\": os.path.join(DATA_DIR, \"Merged_CP.csv\"),\n",
    "}\n",
    "\n",
    "TOP_FEATURES = {\n",
    "    \"Cyber\": [\"time_since_last_packet\", \"wlan.seq\"],\n",
    "    \"Physical\": [\"temperature\", \"temperature\"],\n",
    "    \"Cyber-Physical\": [\"temperature\", \"wlan.seq\"],  # physical + cyber\n",
    "}\n",
    "\n",
    "PALETTE = {\"benign\": \"#66c2a5\", \"dos\": \"#fc8d62\", \"replay\": \"#8da0cb\"}\n",
    "\n",
    "# ---------------- LOAD AND PLOT ----------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, path) in zip(axes, DATASETS.items()):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Normalize class names (important!)\n",
    "    df[\"class\"] = (\n",
    "        df[\"class\"]\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .str.strip()\n",
    "        .str.replace(\"attack\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    f1, f2 = TOP_FEATURES[name]\n",
    "    if f1 not in df.columns or f2 not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Skipping {name}: missing features {f1}, {f2}\")\n",
    "        continue\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=f1,\n",
    "        y=f2,\n",
    "        hue=\"class\",\n",
    "        palette=PALETTE,\n",
    "        s=20,\n",
    "        alpha=0.7,\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"{name} Dataset ‚Äî {f1} vs {f2}\")\n",
    "    ax.set_xlabel(f1)\n",
    "    ax.set_ylabel(f2)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(title=\"Class\", loc=\"best\")\n",
    "\n",
    "plt.suptitle(\"Top 2 Important Features ‚Äî Scatter Plot by Dataset\", fontsize=14, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87f1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create Subplots ===\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "fig.suptitle(\"Distribution of wlan.seq for Each Class (Cyber Dataset)\", fontsize=14)\n",
    "\n",
    "# === Histogram for Benign ===\n",
    "sns.histplot(\n",
    "    data=data_Cyber[data_Cyber[\"class\"] == \"benign\"][\"wlan.seq\"],\n",
    "    bins=30,\n",
    "    ax=axes[0],\n",
    "    kde=False,\n",
    "    color=\"#66c2a5\"\n",
    ")\n",
    "axes[0].set_title(\"Benign\")\n",
    "axes[0].set_xlabel(\"wlan.seq\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# === Histogram for DoS Attack ===\n",
    "sns.histplot(\n",
    "    data=data_Cyber[data_Cyber[\"class\"].str.lower().str.contains(\"dos\")][\"wlan.seq\"],\n",
    "    bins=30,\n",
    "    ax=axes[1],\n",
    "    kde=False,\n",
    "    color=\"#fc8d62\"\n",
    ")\n",
    "axes[1].set_title(\"DoS Attack\")\n",
    "axes[1].set_xlabel(\"wlan.seq\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# === Histogram for Replay Attack ===\n",
    "sns.histplot(\n",
    "    data=data_Cyber[data_Cyber[\"class\"].str.lower().str.contains(\"replay\")][\"wlan.seq\"],\n",
    "    bins=30,\n",
    "    ax=axes[2],\n",
    "    kde=False,\n",
    "    color=\"#8da0cb\"\n",
    ")\n",
    "axes[2].set_title(\"Replay Attack\")\n",
    "axes[2].set_xlabel(\"wlan.seq\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cdfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create Subplots ===\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(16, 4))\n",
    "fig.suptitle(\"Distribution of Flight Time for Each Class (Physical Dataset)\", fontsize=14)\n",
    "\n",
    "# === Histogram for Benign ===\n",
    "sns.histplot(\n",
    "    data=data_Physical[data_Physical[\"class\"] == \"benign\"][\"temperature\"],\n",
    "    bins=30,\n",
    "    ax=axes[0],\n",
    "    kde=False,\n",
    "    color=\"#66c2a5\"\n",
    ")\n",
    "axes[0].set_title(\"Benign\")\n",
    "axes[0].set_xlabel(\"Flight Time\")\n",
    "axes[0].set_ylabel(\"temperature\")\n",
    "\n",
    "# === Histogram for DoS Attack ===\n",
    "sns.histplot(\n",
    "    data=data_Physical[data_Physical[\"class\"].str.lower().str.contains(\"dos\")][\"temperature\"],\n",
    "    bins=30,\n",
    "    ax=axes[1],\n",
    "    kde=False,\n",
    "    color=\"#fc8d62\"\n",
    ")\n",
    "axes[1].set_title(\"DoS Attack\")\n",
    "axes[1].set_xlabel(\"temperature\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "# === Histogram for Replay Attack ===\n",
    "sns.histplot(\n",
    "    data=data_Physical[data_Physical[\"class\"].str.lower().str.contains(\"replay\")][\"temperature\"],\n",
    "    bins=30,\n",
    "    ax=axes[2],\n",
    "    kde=False,\n",
    "    color=\"#8da0cb\"\n",
    ")\n",
    "axes[2].set_title(\"Replay Attack\")\n",
    "axes[2].set_xlabel(\"temperature\")\n",
    "axes[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353bce5",
   "metadata": {},
   "source": [
    "#### Class Distribution Visualization for Cyber , Physical and Cyber-Physical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16180ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# === COLUMN VERIFICATION ===\n",
    "for name, df in [(\"Cyber\", data_Cyber), (\"Physical\", data_Physical), (\"Cyber-Physical\", data_CP)]:\n",
    "    if \"class\" not in df.columns:\n",
    "        raise KeyError(f\"‚ùå The 'class' column is missing in {name}\")\n",
    "\n",
    "# === NORMALIZE CLASS LABELS ===\n",
    "def normalize_labels(y):\n",
    "    \"\"\"Standardize class names for consistent plotting.\"\"\"\n",
    "    return (\n",
    "        y.astype(str)\n",
    "         .str.lower()\n",
    "         .replace(r\"\\s+\", \"\", regex=True)\n",
    "         .replace({\n",
    "             \"dosattack\": \"dos\",\n",
    "             \"dos_attack\": \"dos\",\n",
    "             \"dos\": \"dos\",\n",
    "             \"attackdos\": \"dos\",\n",
    "             \"replay\": \"replay\",\n",
    "             \"replayattack\": \"replay\",\n",
    "             \"replay_attack\": \"replay\",\n",
    "             \"attackreplay\": \"replay\",\n",
    "             \"benign\": \"benign\"\n",
    "         })\n",
    "    )\n",
    "\n",
    "data_Cyber[\"class\"] = normalize_labels(data_Cyber[\"class\"])\n",
    "data_Physical[\"class\"] = normalize_labels(data_Physical[\"class\"])\n",
    "data_CP[\"class\"] = normalize_labels(data_CP[\"class\"])\n",
    "\n",
    "# === CLASS COUNTS ===\n",
    "class_counts_cyber = data_Cyber[\"class\"].value_counts()\n",
    "class_counts_physical = data_Physical[\"class\"].value_counts()\n",
    "class_counts_cp = data_CP[\"class\"].value_counts()\n",
    "\n",
    "# === FIXED COLOR MAP (same order for all) ===\n",
    "class_colors = {\n",
    "    \"benign\": \"#66c2a5\",   # greenish\n",
    "    \"dos\": \"#fc8d62\",      # orange\n",
    "    \"replay\": \"#8da0cb\"    # bluish\n",
    "}\n",
    "\n",
    "# === Function to get colors in class order ===\n",
    "def get_colors(labels):\n",
    "    return [class_colors.get(lbl, \"#cccccc\") for lbl in labels]\n",
    "\n",
    "# === CREATE SUBPLOTS (1 row, 3 columns) ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# --- Pie chart: Cyber ---\n",
    "axes[0].pie(\n",
    "    class_counts_cyber,\n",
    "    labels=class_counts_cyber.index,\n",
    "    colors=get_colors(class_counts_cyber.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[0].set_title(\"Class Distribution ‚Äî Cyber Data\")\n",
    "\n",
    "# --- Pie chart: Physical ---\n",
    "axes[1].pie(\n",
    "    class_counts_physical,\n",
    "    labels=class_counts_physical.index,\n",
    "    colors=get_colors(class_counts_physical.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[1].set_title(\"Class Distribution ‚Äî Physical Data\")\n",
    "\n",
    "# --- Pie chart: Cyber-Physical ---\n",
    "axes[2].pie(\n",
    "    class_counts_cp,\n",
    "    labels=class_counts_cp.index,\n",
    "    colors=get_colors(class_counts_cp.index),\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    "    counterclock=False\n",
    ")\n",
    "axes[2].set_title(\"Class Distribution ‚Äî Cyber-Physical Data\")\n",
    "\n",
    "# === GLOBAL LEGEND ===\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', label=cls.capitalize(),\n",
    "               markerfacecolor=color, markersize=10)\n",
    "    for cls, color in class_colors.items()\n",
    "]\n",
    "fig.legend(handles=handles, loc='lower center', ncol=3, title=\"Class\")\n",
    "\n",
    "# === ADJUSTMENTS ===\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2498740",
   "metadata": {},
   "source": [
    "### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532d2cc",
   "metadata": {},
   "source": [
    "In this step, we performed standardized preprocessing and feature engineering across all UAV datasets. We removed irrelevant or redundant columns (such as timestamps), normalized the class labels to ensure consistent naming across datasets, and applied Min‚ÄìMax scaling to rescale all features between 0 and 1. This allows us to obtain homogeneous feature distributions and avoid scale-related bias in our subsequent modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "input_dir = \"Data\"\n",
    "files = {\n",
    "    \"Physical\": f\"{input_dir}/Merged_Physical.csv\",\n",
    "    \"Cyber\": f\"{input_dir}/Merged_Cyber.csv\",\n",
    "    \"CyberPhysical\": f\"{input_dir}/Merged_CP.csv\"\n",
    "}\n",
    "\n",
    "# === FUNCTION TO NORMALIZE CLASS LABELS ===\n",
    "def normalize_class_labels(y):\n",
    "    \"\"\"\n",
    "    Normalize all class names across datasets:\n",
    "    - lowercase\n",
    "    - remove spaces\n",
    "    - unify naming variants (dos, replay, benign)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        y.astype(str)\n",
    "         .str.lower()\n",
    "         .replace(r\"\\s+\", \"\", regex=True)\n",
    "         .replace({\n",
    "             \"dosattack\": \"dos\",\n",
    "             \"dos_attack\": \"dos\",\n",
    "             \"dos\": \"dos\",\n",
    "             \"attackdos\": \"dos\",\n",
    "             \"replay\": \"replay\",\n",
    "             \"replayattack\": \"replay\",\n",
    "             \"replay_attack\": \"replay\",\n",
    "             \"attackreplay\": \"replay\",\n",
    "             \"benign\": \"benign\"\n",
    "         })\n",
    "    )\n",
    "\n",
    "# === COLUMNS TO DROP PER DATASET TYPE ===\n",
    "drop_map = {\n",
    "    \"Physical\": ['class', 'timestamp_p', 'barometer'],\n",
    "    \"Cyber\": ['class', 'timestamp_c', 'frame.number'],\n",
    "    \"CyberPhysical\": ['class', 'timestamp', 'frame.number', 'barometer']\n",
    "}\n",
    "\n",
    "# === PROCESSING PIPELINE ===\n",
    "processed_data = {}\n",
    "\n",
    "for name, path in files.items():\n",
    "    print(f\"\\nüìÇ Loading {name} dataset ...\")\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"   ‚Üí {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "    # Drop irrelevant columns safely\n",
    "    X = df.drop(columns=[c for c in drop_map[name] if c in df.columns], errors='ignore')\n",
    "\n",
    "    # Normalize class labels\n",
    "    y = normalize_class_labels(df['class'])\n",
    "\n",
    "    # Shuffle (break temporal dependence)\n",
    "    X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "    # Min-Max scaling\n",
    "    scaler = MinMaxScaler() \n",
    "    #scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    processed_data[name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_scaled\": X_scaled,\n",
    "        \"scaler\": scaler\n",
    "    }\n",
    "\n",
    "    print(f\"   ‚úÖ Processed: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "\n",
    "# === SUMMARY OF NORMALIZED CLASS DISTRIBUTIONS ===\n",
    "print(\"\\n=== üìä Normalized Class Distributions ===\")\n",
    "for name, data in processed_data.items():\n",
    "    counts = data[\"y\"].value_counts().to_dict()\n",
    "    print(f\"{name:15s} ‚Üí {counts}\")\n",
    "\n",
    "# === EXTRACT VARIABLES FOR MODELING ===\n",
    "X_physical, X_physical_scaled, y_physical = (\n",
    "    processed_data[\"Physical\"][\"X\"],\n",
    "    processed_data[\"Physical\"][\"X_scaled\"],\n",
    "    processed_data[\"Physical\"][\"y\"]\n",
    ")\n",
    "\n",
    "X_cyber, X_cyber_scaled, y_cyber = (\n",
    "    processed_data[\"Cyber\"][\"X\"],\n",
    "    processed_data[\"Cyber\"][\"X_scaled\"],\n",
    "    processed_data[\"Cyber\"][\"y\"]\n",
    ")\n",
    "\n",
    "X_cp, X_cp_scaled, y_cp = (\n",
    "    processed_data[\"CyberPhysical\"][\"X\"],\n",
    "    processed_data[\"CyberPhysical\"][\"X_scaled\"],\n",
    "    processed_data[\"CyberPhysical\"][\"y\"]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ All datasets successfully loaded and preprocessed!\")\n",
    "print(f\"Physical: X={X_physical.shape}, y={y_physical.shape}\")\n",
    "print(f\"Cyber:    X={X_cyber.shape}, y={y_cyber.shape}\")\n",
    "print(f\"Cyber-Physical: X={X_cp.shape}, y={y_cp.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832dcfbb",
   "metadata": {},
   "source": [
    "### **Machine Learning Models - Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06b454",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7200d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1Ô∏è‚É£ PCA FIT ===\n",
    "pca_cyber = PCA().fit(X_cyber_scaled)\n",
    "pca_physical = PCA().fit(X_physical_scaled)\n",
    "pca_cp = PCA().fit(X_cp_scaled)\n",
    "\n",
    "# === 2Ô∏è‚É£ Cumulative Explained Variance ===\n",
    "var_cum_cyber = np.cumsum(pca_cyber.explained_variance_ratio_)\n",
    "var_cum_physical = np.cumsum(pca_physical.explained_variance_ratio_)\n",
    "var_cum_cp = np.cumsum(pca_cp.explained_variance_ratio_)\n",
    "\n",
    "# === 3Ô∏è‚É£ Plot cumulative variance (3 side-by-side graphs) ===\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# ---- Plot 1: Cyber Dataset ----\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(var_cum_cyber, marker='o', color='blue')\n",
    "plt.title(\"PCA - Cyber Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "# ---- Plot 2: Physical Dataset ----\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(var_cum_physical, marker='o', color='orange')\n",
    "plt.title(\"PCA - Physical Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "# ---- Plot 3: Cyber-Physical Dataset ----\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(var_cum_cp, marker='o', color='green')\n",
    "plt.title(\"PCA - Cyber-Physical Dataset\")\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 4Ô∏è‚É£ PCA with 2 components ===\n",
    "pca2_cyber = PCA(n_components=2)\n",
    "X_cyber_pca2 = pca2_cyber.fit_transform(X_cyber_scaled)\n",
    "\n",
    "pca2_physical = PCA(n_components=2)\n",
    "X_physical_pca2 = pca2_physical.fit_transform(X_physical_scaled)\n",
    "\n",
    "pca2_cp = PCA(n_components=2)\n",
    "X_cp_pca2 = pca2_cp.fit_transform(X_cp_scaled)\n",
    "\n",
    "# === 5Ô∏è‚É£ Define classes and colors ===\n",
    "colors = ['r', 'b', 'g', 'purple', 'orange']\n",
    "\n",
    "classes_cyber = np.unique(y_cyber)\n",
    "classes_physical = np.unique(y_physical)\n",
    "classes_cp = np.unique(y_cp)\n",
    "\n",
    "# === 6Ô∏è‚É£ 2D PCA Visualization (3 side-by-side graphs) ===\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# ---- PCA Cyber ----\n",
    "plt.subplot(1, 3, 1)\n",
    "for i, cls in enumerate(classes_cyber):\n",
    "    plt.scatter(\n",
    "        X_cyber_pca2[y_cyber == cls, 0],\n",
    "        X_cyber_pca2[y_cyber == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Cyber Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- PCA Physical ----\n",
    "plt.subplot(1, 3, 2)\n",
    "for i, cls in enumerate(classes_physical):\n",
    "    plt.scatter(\n",
    "        X_physical_pca2[y_physical == cls, 0],\n",
    "        X_physical_pca2[y_physical == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Physical Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# ---- PCA Cyber-Physical ----\n",
    "plt.subplot(1, 3, 3)\n",
    "for i, cls in enumerate(classes_cp):\n",
    "    plt.scatter(\n",
    "        X_cp_pca2[y_cp == cls, 0],\n",
    "        X_cp_pca2[y_cp == cls, 1],\n",
    "        c=colors[i % len(colors)],\n",
    "        label=cls,\n",
    "        alpha=0.6,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "plt.title(\"PCA Projection (2D) ‚Äî Cyber-Physical Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffcb21c",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameter grids for each kernel ===\n",
    "param_grids = {\n",
    "    \"linear\": {\n",
    "        \"C\": [0.1, 1, 10]\n",
    "    },\n",
    "    \"poly\": {\n",
    "        \"C\": [0.1, 1],\n",
    "        \"degree\": [2, 3, 4, 5],\n",
    "    },\n",
    "    \"rbf\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [1e-2, 1e-1, 1, 10]\n",
    "    },\n",
    "    \"sigmoid\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"gamma\": [\"scale\", \"auto\"],\n",
    "        \"coef0\": [0, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Store results ===\n",
    "results = {\"cyber\": {}, \"physical\": {}, \"cyber_physical\": {}}\n",
    "\n",
    "# === Loop through all three datasets ===\n",
    "for dataset in [\"cyber\", \"physical\", \"cyber_physical\"]:\n",
    "\n",
    "    print(f\"\\n\\n==================== üß© DATASET: {dataset.upper()} ====================\")\n",
    "\n",
    "    # --- Dataset configuration ---\n",
    "    if dataset == \"cyber\":\n",
    "        n_half = len(X_cyber_scaled) // 4\n",
    "        X_scaled = X_cyber_scaled[:n_half]\n",
    "        y = y_cyber[:n_half]\n",
    "        n_comp = 4\n",
    "        print(f\"üìâ Cyber sub-sampling: keeping {n_half} rows\")\n",
    "    elif dataset == \"physical\":\n",
    "        X_scaled = X_physical_scaled\n",
    "        y = y_physical\n",
    "        n_comp = 5\n",
    "    else:  # cyber_physical\n",
    "        X_scaled = X_cp_scaled\n",
    "        y = y_cp\n",
    "        n_comp = 8\n",
    "\n",
    "    # --- PCA dimensionality reduction ---\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    print(f\"üîπ {dataset.capitalize()}: variance explained by {n_comp} components = {np.sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "    # --- Train/test split ---\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_pca, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # --- Loop over kernel types ---\n",
    "    for kernel_choice, param_grid in param_grids.items():\n",
    "        print(f\"\\n=== ‚öôÔ∏è GridSearch for kernel = '{kernel_choice}' ===\")\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            SVC(kernel=kernel_choice, decision_function_shape='ovr'),\n",
    "            param_grid=param_grid,\n",
    "            scoring='accuracy',\n",
    "            cv=3,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            refit=True\n",
    "        )\n",
    "\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_score = grid.best_score_\n",
    "        best_params = grid.best_params_\n",
    "        # ajouter k-fold cross-validation\n",
    "        print(f\"‚úÖ Best parameters: {best_params}\")\n",
    "        print(f\"üèÅ Cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "        # --- Final training ---\n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        acc_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"üéØ Test accuracy: {acc_test:.4f}\")\n",
    "        results[dataset][kernel_choice] = acc_test\n",
    "\n",
    "# === Summary of results ===\n",
    "print(\"\\n==================== üèÅ SUMMARY OF TEST ACCURACIES ====================\")\n",
    "for ds, scores in results.items():\n",
    "    print(f\"\\nüìä {ds.upper()}:\")\n",
    "    for k, s in scores.items():\n",
    "        print(f\"  {k:10s} : {s:.4f}\")\n",
    "\n",
    "# === Visualization ===\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "kernels = list(param_grids.keys())\n",
    "x = np.arange(len(kernels))\n",
    "width = 0.25\n",
    "\n",
    "cyber_scores = [results[\"cyber\"].get(k, 0) for k in kernels]\n",
    "phys_scores = [results[\"physical\"].get(k, 0) for k in kernels]\n",
    "cp_scores = [results[\"cyber_physical\"].get(k, 0) for k in kernels]\n",
    "\n",
    "bars1 = ax.bar(x - width, cyber_scores, width, label='Cyber', alpha=0.8)\n",
    "bars2 = ax.bar(x, phys_scores, width, label='Physical', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, cp_scores, width, label='Cyber-Physical', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Kernel type\")\n",
    "ax.set_ylabel(\"Test accuracy\")\n",
    "ax.set_title(\"SVM Performance Comparison ‚Äî Cyber vs Physical vs Cyber-Physical\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(kernels)\n",
    "ax.legend()\n",
    "ax.bar_label(bars1, fmt='%.2f', padding=3)\n",
    "ax.bar_label(bars2, fmt='%.2f', padding=3)\n",
    "ax.bar_label(bars3, fmt='%.2f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98890d",
   "metadata": {},
   "source": [
    "#### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae88523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "datasets = {\n",
    "    \"Cyber\": (X_cyber, y_cyber),\n",
    "    \"Physical\": (X_physical, y_physical),\n",
    "    \"Cyber-Physical\": (X_cp, y_cp)\n",
    "}\n",
    "\n",
    "n_estimators = 200\n",
    "random_state = 42\n",
    "k_folds = 5\n",
    "\n",
    "# === STORE RESULTS ===\n",
    "per_class_metrics = []\n",
    "kfold_results = {}\n",
    "\n",
    "# === LOOP OVER DATASETS ===\n",
    "for name, (X, y) in datasets.items():\n",
    "    X, y = shuffle(X, y, random_state=random_state)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.25, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion='entropy', random_state=random_state)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    for cls in report.keys():\n",
    "        if cls in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "            continue\n",
    "        per_class_metrics.append({\n",
    "            \"Dataset\": name,\n",
    "            \"Class\": cls.lower(),\n",
    "            \"Accuracy\": report[cls][\"precision\"],\n",
    "            \"Recall\": report[cls][\"recall\"],\n",
    "            \"F1\": report[cls][\"f1-score\"]\n",
    "        })\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "    scores = cross_val_score(rf, X_scaled, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "    kfold_results[name] = scores\n",
    "\n",
    "df_class_metrics = pd.DataFrame(per_class_metrics)\n",
    "\n",
    "# === COLORS BY CLASS ===\n",
    "class_colors = {\n",
    "    \"benign\": \"#66c2a5\",\n",
    "    \"dos\": \"#fc8d62\",\n",
    "    \"replay\": \"#8da0cb\"\n",
    "}\n",
    "\n",
    "# === PLOTS FOR PER-CLASS METRICS SIDE BY SIDE ===\n",
    "# === PLOTS FOR PER-CLASS METRICS SIDE BY SIDE ===\n",
    "metrics = [(\"Accuracy\", \"Accuracy (Precision)\"),\n",
    "           (\"Recall\", \"Recall\"),\n",
    "           (\"F1\", \"F1-score\")]\n",
    "\n",
    "datasets_order = [\"Cyber\", \"Physical\", \"Cyber-Physical\"]\n",
    "class_list = list(class_colors.keys())\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(datasets_order))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5), sharey=True)\n",
    "\n",
    "for ax, (metric_name, ylabel) in zip(axes, metrics):\n",
    "    for i, cls in enumerate(class_list):\n",
    "        subset = df_class_metrics[df_class_metrics[\"Class\"].str.lower() == cls]\n",
    "        subset = subset.set_index(\"Dataset\").reindex(datasets_order)\n",
    "        ax.bar(\n",
    "            x + i * bar_width,\n",
    "            subset[metric_name],\n",
    "            width=bar_width,\n",
    "            label=cls.capitalize() if metric_name == \"Accuracy\" else \"\",\n",
    "            color=class_colors[cls]\n",
    "        )\n",
    "        for j, val in enumerate(subset[metric_name]):\n",
    "            if not np.isnan(val):\n",
    "                ax.text(x[j] + i * bar_width, val + 0.01, f\"{val:.2f}\", ha='center', fontsize=9)\n",
    "\n",
    "    ax.set_xticks(x + bar_width)\n",
    "    ax.set_xticklabels(datasets_order)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_title(metric_name)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "fig.legend(title=\"Class\", loc='upper center', ncol=len(class_list))\n",
    "fig.suptitle(\"Random Forest ‚Äî Performance Metrics by Class and Dataset\", fontsize=13, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === FEATURE IMPORTANCES PLOTS (side by side for the 3 datasets) ===\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, name in zip(axes, datasets_order):\n",
    "    X, y = datasets[name]\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, criterion='entropy', random_state=random_state)\n",
    "    rf.fit(X_scaled, y)\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_n = min(10, len(importances))  # top 10 features or less if fewer exist\n",
    "\n",
    "    ax.barh(range(top_n), importances[indices[:top_n]][::-1], color=\"#66c2a5\")\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(np.array(X.columns)[indices[:top_n]][::-1], fontsize=9)\n",
    "    ax.set_title(f\"{name} ‚Äî Top {top_n} Feature Importances\")\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "fig.suptitle(\"Random Forest ‚Äî Top Feature Importances per Dataset\", fontsize=13, y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f2d8",
   "metadata": {},
   "source": [
    "#### Anomaly Detection: Novelty Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eb7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "#        UAVs Dataset ‚Äî Novelty Detection Optimization\n",
    "# =====================================================\n",
    "#  ‚úÖ Keep only Novelty Detection\n",
    "#  ‚úÖ Add GridSearchCV + KFold\n",
    "#  ‚úÖ Evaluate IF and LOF on 3 datasets\n",
    "# =====================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------- CONFIGURATION ----------------\n",
    "datasets = {\n",
    "    \"Cyber\": (X_cyber, y_cyber),\n",
    "    \"Physical\": (X_physical, y_physical),\n",
    "    \"Cyber-Physical\": (X_cp, y_cp)\n",
    "}\n",
    "\n",
    "contaminations = {\n",
    "    \"Cyber\": 0.5,\n",
    "    \"Physical\": 0.32,\n",
    "    \"Cyber-Physical\": 0.32\n",
    "}\n",
    "\n",
    "random_state = 42\n",
    "k_folds = 5\n",
    "\n",
    "# ---------------- SCORERS ----------------\n",
    "def novelty_f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, pos_label=-1)\n",
    "\n",
    "def novelty_prec(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, pos_label=-1)\n",
    "\n",
    "def novelty_rec(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=-1)\n",
    "\n",
    "scorers = {\n",
    "    \"F1\": make_scorer(novelty_f1),\n",
    "    \"Precision\": make_scorer(novelty_prec),\n",
    "    \"Recall\": make_scorer(novelty_rec)\n",
    "}\n",
    "\n",
    "# ---------------- GRID SEARCH FUNCTION ----------------\n",
    "def run_novelty_search(X, y, dataset_name, contamination):\n",
    "    print(f\"\\n=== {dataset_name.upper()} ‚Äî Novelty Detection (contamination={contamination}) ===\")\n",
    "\n",
    "    mask_benign = (y == \"benign\")\n",
    "    X_train = X[mask_benign]\n",
    "    X_test = X\n",
    "    y_true = np.where(y == \"benign\", 1, -1)\n",
    "\n",
    "    # --- Define parameter grids ---\n",
    "    param_grid_if = {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_samples\": [\"auto\", 0.8],\n",
    "        \"contamination\": [contamination],\n",
    "    }\n",
    "\n",
    "    param_grid_lof = {\n",
    "        \"n_neighbors\": [10, 20, 30],\n",
    "        \"contamination\": [contamination],\n",
    "        \"leaf_size\": [20, 30, 40]\n",
    "    }\n",
    "\n",
    "    # --- Define models ---\n",
    "    models = {\n",
    "        \"Isolation Forest\": (IsolationForest(random_state=random_state), param_grid_if),\n",
    "        \"Local Outlier Factor\": (LocalOutlierFactor(novelty=True), param_grid_lof)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- Run GridSearchCV with K-Fold ---\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for name, (model, grid) in models.items():\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=grid,\n",
    "            scoring=scorers,\n",
    "            refit=\"F1\",\n",
    "            cv=kf,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train, y=None)\n",
    "\n",
    "        best_model = grid_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        f1 = novelty_f1(y_true, y_pred)\n",
    "        prec = novelty_prec(y_true, y_pred)\n",
    "        rec = novelty_rec(y_true, y_pred)\n",
    "\n",
    "        print(f\"üîπ {name} ‚Äî Best params: {grid_search.best_params_}\")\n",
    "        print(f\"   ‚Üí F1={f1:.3f}, Precision={prec:.3f}, Recall={rec:.3f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Algo\": name.replace(\"Isolation Forest\", \"IF\").replace(\"Local Outlier Factor\", \"LOF\"),\n",
    "            \"F1\": f1,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ---------------- RUN ALL DATASETS ----------------\n",
    "df_all = pd.concat([\n",
    "    run_novelty_search(X_cyber, y_cyber, \"Cyber\", contaminations[\"Cyber\"]),\n",
    "    run_novelty_search(X_physical, y_physical, \"Physical\", contaminations[\"Physical\"]),\n",
    "    run_novelty_search(X_cp, y_cp, \"Cyber-Physical\", contaminations[\"Cyber-Physical\"]),\n",
    "], ignore_index=True)\n",
    "# ---------------- VISUALIZATION ----------------\n",
    "metrics = [\"Precision\", \"Recall\", \"F1\"]\n",
    "datasets_order = [\"Cyber\", \"Physical\", \"Cyber-Physical\"]\n",
    "algos = [\"IF\", \"LOF\"]\n",
    "colors = {\"Cyber\": \"#66c2a5\", \"Physical\": \"#fc8d62\", \"Cyber-Physical\": \"#8da0cb\"}\n",
    "bar_width = 0.25\n",
    "x = np.arange(len(algos))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "fig.suptitle(\"Novelty Detection (GridSearch + KFold)\", fontsize=15, weight=\"bold\")\n",
    "\n",
    "for ax, metric in zip(axes, metrics):\n",
    "    for i, dataset in enumerate(datasets_order):\n",
    "        subset = df_all[df_all[\"Dataset\"] == dataset].set_index(\"Algo\").reindex(algos)\n",
    "        ax.bar(\n",
    "            x + i * bar_width,\n",
    "            subset[metric],\n",
    "            width=bar_width,\n",
    "            label=dataset if metric == \"Precision\" else \"\",\n",
    "            color=colors[dataset]\n",
    "        )\n",
    "        for j, val in enumerate(subset[metric]):\n",
    "            if not np.isnan(val):\n",
    "                ax.text(x[j] + i * bar_width, val + 0.01, f\"{val:.2f}\", ha='center', fontsize=9)\n",
    "\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticks(x + bar_width)\n",
    "    ax.set_xticklabels(algos)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "axes[0].set_ylabel(\"Score\")\n",
    "fig.legend(title=\"Dataset\", loc=\"upper center\", ncol=3)\n",
    "plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690043a1",
   "metadata": {},
   "source": [
    "### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4a888",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
